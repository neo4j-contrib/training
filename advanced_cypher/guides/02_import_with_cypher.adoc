= Importing data with Cypher
:csv_url: file://
// :csv_url: http://data.neo4j.com.s3.amazonaws.com/advanced/movies
:movies_roles_url: '{csv_url}/movies.csv'
:movies_url: '{csv_url}/new_movies.csv'
:people_url: '{csv_url}/people.csv'
:actors_url: '{csv_url}/actors.csv'
:directors_url: '{csv_url}/directors.csv'

== LOAD CSV - Data Inspection

Now that we've got an overview of the CSV files it's time to take a closer look at them.
We're going to use the `LOAD CSV` command to do exploration.

The following are some issues we've seen with CSV files:

- incorrect header vs. data
- wrong quotes, unquoted newlines, stray quotes
- UTF-8 prefix
- trailing spaces, typos, binary zeros

Even if everything looks fine using our command line tools it's still good to see how Cypher sees those same files.

== Row Count

We'll start with a simple count of the number of lines:

[source,cypher,subs=attributes]
----
LOAD CSV FROM {movies_url} AS row
RETURN count(*);
----

If you haven't been able to get the CSV files working locally, don't worry!
If you replace `file:///` with `http://data.neo4j.com.s3.amazonaws.com/advanced/movies/` you can process the CSV files from our S3 bucket.

== Row as List Data

[source,cypher,subs=attributes]
----
LOAD CSV FROM {movies_url} AS row
RETURN *
LIMIT 5
----

== LOAD CSV - WITH HEADERS

If we include `WITH HEADERS` along with the `LOAD CSV` command we'll get back a map per row.
Let's try that out.

== Row as Map Data

[source,cypher,subs=attributes]
----
LOAD CSV WITH HEADERS FROM {movies_url} AS row
RETURN row, keys(row)
LIMIT 5
----

== Data Conversion

By default data is imported as strings but we can coerce the data using the `toInteger` and `toFloat` functions.
The `split` function is useful for processing

[source,cypher,subs=attributes]
----
LOAD CSV WITH HEADERS FROM {movies_url} AS row
RETURN toFloat(row.avgVote) as avgVote, toInteger(row.releaseYear) as releaseYear, split(row.genres,":") as genres, row
LIMIT 5;
----

== Exercise: Creating Data (10')

Find your favourite actor and a movie they acted in from the CSV files and manually `CREATE` the `Movie` and `MERGE` the `Person.

NOTE: Look at the CSV structure for property-names and types.

== Solution on next page

== Answer: Creating Data

[source,cypher]
----
CREATE (:Movie {
          id:5574,
          title:"Forrest Gump",
          tagline: "",
          avgVote:7.7,
          releaseYear:1994,
          genres: ["Comedy","Drama","Romance"] })
----

[source,cypher]
----
MERGE (p:Person {id: 31})
ON CREATE SET p.name = "Tom Hanks", p.birthYear = 1956
----

[source,cypher]
----
MATCH (p:Person {id:31}), (m:Movie {id:5574})
CREATE (p)-[:ACTED_IN {roles:['Forrest Gump']}]->(m)
----

== LOAD CSV - Prep

Create indexes & constraints esp. for relationship creation.

[source,cypher]
----
CREATE CONSTRAINT ON (m:Movie)
ASSERT m.id IS UNIQUE
----
[source,cypher]
----
CREATE CONSTRAINT ON (p:Person)
ASSERT p.id IS UNIQUE
----

[source,cypher]
----
CREATE INDEX ON :Person(name)
----

[source,cypher]
----
CREATE INDEX ON :Movie(title)
----

== LOAD CSV - Import Approaches

* normalized data
* denormalized data - multi-pass
* denormalized data - single-pass
* optimizations

== Normalized Data

* Separate CSV files
* Create nodes individually, one per label
* Create relationships, one per type

== Import Movies

[source,cypher,subs=attributes]
----
LOAD CSV WITH HEADERS FROM {movies_url} AS row
CREATE (:Movie {
          id:toInteger(row.movieId),
          title:row.title,
          avgVote:toFloat(row.avgVote),
          releaseYear:toInteger(row.releaseYear),
          genres: split(row.genres,":") })
----

Data Transformation from string

* to integer for releaseYear
* to floating point for avgVote
* to a list for genres

== Exercise: Import People (10')

* Import the people from {people_url}
* Determine the number of rows
* Determine structure of first 5 rows
* Goal: `:Person(id,name,born,died)`
* Import people with `CREATE`
* Re-run with `MERGE`

Make sure to transform the time

== Exercise: Import People

Solution on next slide, don't peek.

== Import People

[source,cypher,subs=attributes]
----
LOAD CSV WITH HEADERS FROM {people_url} as row

MERGE(person:Person {id: toInteger(row.personId)})
ON CREATE SET person.name = row.name,
              person.born = toInteger(row.birthYear),
              person.died = toInteger(row.deathYear)
----

NOTE: `deathYear` can be missing. `toInteger()` returns null, property not set.

== Import Directors

[source, cypher, subs=attributes]
----
LOAD CSV WITH HEADERS FROM {directors_url} as row

MATCH (movie:Movie {id:toInteger(row.movieId)})
MATCH (person:Person {id: toInteger(row.personId)})
MERGE (person)-[:DIRECTED]->(movie)
ON CREATE SET person:Director
----


== Exercise: Import Actors

* Import `ACTED_IN(roles)` relationship
* From {actors_url}
* Determine rows and structure of first 5 entries
* Only create one ACTED_IN relationship per Person->Movie
* Set `roles` to a list of roles
* Set `:Actor` label

== Exercise: Import Actors

Solution on next page. Wait here.

== Solution: Import Actors

[source,cypher,subs=attributes]
----
USING PERIODIC COMMIT 50000
LOAD CSV WITH HEADERS FROM {actors_url} AS row
FIELDTERMINATOR ','

MATCH  (movie:Movie  {id: toInteger(row.movieId) })
MATCH  (person:Person {id: toInteger(row.personId) })
MERGE  (person)-[r:ACTED_IN]->(movie) ON CREATE SET r.roles = split(coalesce(row.characters,""), ":")
ON CREATE SET person:Actor
----

Discuss:

* FIELDTERMINATOR
* PERIODIC COMMIT
* Eagerness

== Pro

* Simple statements
* Single merge for movies and actors
* Single Pass

== Con

* Additional index lookups
* Deadlocks for relationships if parallelized

== Denormalized Data (1)

* Single CSV file
* *Multi-Pass*
* Create nodes individually, one per label
* Create relationships, one per type

Same as before, just run multiple passes over the same file.

== Pro

* Simple statement, easy to understand

== Con

* Unnecessary merges for duplicate movies and actors
* Additional index lookups
* Multi Pass
* Deadlocks for rels if parallelized

== Denormalized Data (2)

* Single denormalized CSV file
* Single-Pass
* Create sub-graph per row, e.g. Movie and Person and Relationship

[source,cypher,subs=attributes]
----
LOAD CSV WITH HEADERS FROM {movies_roles_url} AS row

MERGE (m:Movie {id:toInteger(row.movieId)})
   ON CREATE SET m.title=row.title, m.avgVote=toFloat(row.avgVote),
      m.releaseYear=toInteger(row.releaseYear), m.genres=split(row.genres,":")

MERGE (p:Person {id: toInteger(row.personId)})
   ON CREATE SET p.name = row.name, p.born = toInteger(row.birthYear),
      p.died = toInteger(row.deathYear)

MERGE (p)-[:ACTED_IN {roles: split(row.characters,':')}]->(m)
ON CREATE SET p:Actor;
----

== Pro

* Saves index lookups
* Single Pass
* Works well with cost based planner

== Con

* More complex statement
* Unnecessary merges for duplicate movies and actors
* Deadlocks if parallelized
* Potential Eagerness problem

== Reduce Index lookups

* Small datasets (<1M) also work *without* PERIODIC COMMIT. Test it.
* Use distinct with input data, can use CREATE instead of MERGE
* MERGE has fewer lookups

[source,cypher,subs=attributes]
----
LOAD CSV WITH HEADERS FROM {movies_roles_url} AS row

WITH DISTINCT toInteger(row.movieId) as movieId, row.title as title, row.genres as genres,
toInteger(row.releaseYear) as releaseYear, toFloat(row.avgVote) as avgVote

MERGE (m:Movie {id:movieId)
   ON CREATE SET m.title=title, m.avgVote=avgVote,
      m.releaseYear=toInteger(row.releaseYear), m.genres=split(genres,":")

----

== Recovering if you messed up (with Periodic Commit)

* Mark newly created data with label (relationships with property) in (ON) CREATE
* Remove nodes with that label / rels with that property

* In Neo4j-Shell / Cypher-Shell use begin/rollback transactions

== Aggregate sub-structure

* Reduce Index-Lookup for Movie
* Turns statement to be (artificially) eager
* Effectively disables periodic commit

[source,cypher,subs=attributes]
----
PROFILEs
LOAD CSV WITH HEADERS FROM {movies_roles_url} AS row

WITH row.movieId as movieId, row.title as title, row.genres as genres,
toInteger(row.releaseYear) as releaseYear, toFloat(row.avgVote) as avgVote,

collect({id: row.personId, name:row.name, born: toInteger(row.birthYear), died:toInteger(row.deathYear),
         roles: split(coalesce(row.characters,""),':')}) as people

RETURN * LIMIT 10;
----

[source,cypher,subs=attributes]
----
LOAD CSV WITH HEADERS FROM {movies_roles_url} AS row

WITH row.movieId as movieId, row.title as title, row.genres as genres,
toInteger(row.releaseYear) as releaseYear, toFloat(row.avgVote) as avgVote,

collect({id: row.personId, name:row.name, born: toInteger(row.birthYear), died:toInteger(row.deathYear),
         roles: split(coalesce(row.characters,""),':')}) as people

MERGE (m:Movie {id:movieId)
   ON CREATE SET m.title=title, m.avgVote=avgVote,
      m.releaseYear=toInteger(row.releaseYear), m.genres=split(genres,":")

UNWIND people as person

MERGE (p:Person {id: person.id})
   ON CREATE SET p.name = person.name, p.born = person.born, p.died = person.died

CREATE (p)-[:ACTED_IN {roles: person.roles}]->(m);
----

////
== LOAD CSV today (create small subgraphs vs. nodes then rels)
- we used to convey that you have to strictly create nodes first (separately)
- and only then relationships
- today with the better eager handling and cost based writes
- I think you can actually create sensible subgraphs (let's say up to 100 or 1000 nodes) per row
- that should also help with concurrent execution and deadlocks
- start with creating / updating the root node of your subgraph to take a lock

== Cost planner for WRITES what changed
- now that we have the cost planner for writes, what has changed
- e.g. demo decomposition of a MERGE or MERGE relationship
- more sensible matches for long patterns or varlength
- so it enables more complex create patterns again
- eager is also better

== Next step

pass:a[<a play-topic={guides}/03_aggregates.html'>Aggregate Queries</a>]
////
